#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¢æ¥éŸ³å£°åˆ†æãƒ—ãƒ­ã‚°ãƒ©ãƒ  (Interview Speech Analyzer)

ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯é¢æ¥éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆm4aå½¢å¼ï¼‰ã‚’åˆ†æã—ã¦ã€ä»¥ä¸‹ã®è©•ä¾¡æŒ‡æ¨™ã‚’ç®—å‡ºã—ã¾ã™ï¼š
1. ãƒã‚¸ãƒ†ã‚£ãƒ– vs ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã®æ¯”ç‡
2. èºŠèº‡ç‡ï¼ˆãƒ•ã‚£ãƒ©ãƒ¼é »åº¦ï¼‰
3. ç²˜ã‚Šå¼·ã•ã®è¡¨ç¾é »åº¦
4. è³ªå•å¿œç­”é€Ÿåº¦
5. æ¥ç¶šè©ä½¿ç”¨é »åº¦
6. ç™ºè©±é€Ÿåº¦ï¼ˆWPMï¼šWords Per Minuteï¼‰
7. ä¸€äººç§°/ä¸‰äººç§°æ¯”ç‡

ä½¿ç”¨æ–¹æ³•ï¼š
1. inputãƒ•ã‚©ãƒ«ãƒ€ã«åˆ†æã—ãŸã„m4aãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€Œinterview.m4aã€ã¨ã„ã†åå‰ã§é…ç½®
2. ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè¡Œï¼š python interview_analyzer.py
3. outputãƒ•ã‚©ãƒ«ãƒ€ã«åˆ†æçµæœï¼ˆã‚°ãƒ©ãƒ•ç”»åƒã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ãŒä¿å­˜ã•ã‚Œã¾ã™

æ³¨æ„ï¼š
- åˆå›å®Ÿè¡Œæ™‚ã¯Whisperãƒ¢ãƒ‡ãƒ«ã‚„SpeechBrainãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…è¦ã§ã™
- Python 3.8ä»¥ä¸ŠãŒå¿…è¦ã§ã™

ä½œæˆè€…: [ã‚ãªãŸã®åå‰]
æ›´æ–°æ—¥: 2024å¹´
"""

# ==============================================================================
# 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ==============================================================================

import os
import sys
import time
import tempfile
import yaml
import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter, defaultdict
import platform  # OSã‚’åˆ¤å®šã™ã‚‹ãŸã‚ã«è¿½åŠ 
import matplotlib  # rcParamsã‚’ç›´æ¥è¨­å®šã™ã‚‹ãŸã‚ã«è¿½åŠ 

# éŸ³å£°å‡¦ç†é–¢é€£
from pydub import AudioSegment
import torch
import torchaudio
import whisper

# è‡ªç„¶è¨€èªå‡¦ç†é–¢é€£
from janome.tokenizer import Tokenizer

# è©±è€…èªè­˜é–¢é€£
from speechbrain.pretrained import SpeakerRecognition
from sklearn.cluster import AgglomerativeClustering

# å¯è¦–åŒ–é–¢é€£
import matplotlib.pyplot as plt
import seaborn as sns
import japanize_matplotlib

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
from tqdm.auto import tqdm

# SSLè¨¼æ˜æ›¸é–¢é€£
import ssl
import certifi

# ==============================================================================
# 2. åˆæœŸè¨­å®š
# ==============================================================================

# SSLè¨¼æ˜æ›¸ã®è¨­å®šï¼ˆwhisperãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰
ssl._create_default_https_context = ssl._create_unverified_context
os.environ['SSL_CERT_FILE'] = certifi.where()

# CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã®åˆ¶é™ï¼ˆå®‰å®šæ€§å‘ä¸Šã®ãŸã‚ï¼‰
os.environ["OMP_NUM_THREADS"] = "1"
torch.set_num_threads(1)

# ã‚°ãƒ©ãƒ•ã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# --- â–¼ã“ã“ã‹ã‚‰ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’è¿½åŠ  â–¼ ---
try:
    system_name = platform.system()
    if system_name == "Darwin":  # macOSã®å ´åˆ
        # ãƒ’ãƒ©ã‚®ãƒãƒ•ã‚©ãƒ³ãƒˆã‚’æŒ‡å®šã€‚ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹æ­£ç¢ºãªåå‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
        # ä¸€èˆ¬çš„ãªãƒ’ãƒ©ã‚®ãƒãƒ•ã‚©ãƒ³ãƒˆã®å€™è£œã‚’å„ªå…ˆé †ã«ãƒªã‚¹ãƒˆã—ã¾ã™ã€‚
        matplotlib.rcParams['font.family'] = ['Hiragino Sans', 'Hiragino Kaku Gothic ProN', 'IPAexGothic', 'sans-serif']
    elif system_name == "Windows":  # Windowsã®å ´åˆ
        # Meiryoã‚„Yu Gothicãªã©ã‚’æŒ‡å®š
        matplotlib.rcParams['font.family'] = ['Meiryo', 'Yu Gothic', 'MS Gothic', 'IPAexGothic', 'sans-serif']
    else:  # Linuxã‚„ãã®ä»–ã®OSã®å ´åˆ
        # japanize-matplotlibãŒIPAexGothicã‚’é©åˆ‡ã«è¨­å®šã™ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¤ã¤ã€
        # ä»–ã®ãƒ•ã‚©ãƒ³ãƒˆã‚‚å€™è£œã«å…¥ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
        matplotlib.rcParams['font.family'] = ['IPAexGothic', 'Noto Sans CJK JP', 'TakaoPGothic', 'sans-serif']
    
    print(f"OS: {system_name}, è¨­å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ãƒŸãƒªãƒ¼: {matplotlib.rcParams['font.family']}")

except Exception as e:
    print(f"ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã«å¤±æ•—ã—ãŸå ´åˆã§ã‚‚ã€japanize-matplotlibã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æœŸå¾…ã—ã¾ã™ã€‚
    pass
# --- â–²ã“ã“ã¾ã§ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’è¿½åŠ  â–² ---

# ==============================================================================
# 3. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å®šç¾©
# ==============================================================================

# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
AUDIO_M4A = Path("input/interview.m4a")  # åˆ†æå¯¾è±¡ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«
PATTERNS_YAML = Path("database/speech_patterns.yaml")  # è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
OUTPUT_DIR = Path("output")
CACHE_DIR = Path("cache")

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆéŸ³å£°å¤‰æ›ç”¨ï¼‰
WAV_TMP = Path(tempfile.gettempdir()) / "interview_temp.wav"

# ==============================================================================
# 4. ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ã®å®šç¾©
# ==============================================================================

class InterviewSpeechAnalyzer:
    """
    é¢æ¥éŸ³å£°åˆ†æã‚¯ãƒ©ã‚¹
    
    ã“ã®ã‚¯ãƒ©ã‚¹ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š
    1. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    2. éŸ³å£°ã®æ–‡å­—èµ·ã“ã—ï¼ˆWhisperä½¿ç”¨ï¼‰
    3. è©±è€…ã®è­˜åˆ¥ï¼ˆé¢æ¥å®˜ã¨é¢æ¥è€…ã®åŒºåˆ¥ï¼‰
    4. è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
    5. è©•ä¾¡æŒ‡æ¨™ã®ç®—å‡º
    6. çµæœã®å¯è¦–åŒ–ã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    """
    
    def __init__(self):
        """
        ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        å¿…è¦ãªè¨­å®šã¨ãƒ„ãƒ¼ãƒ«ã‚’æº–å‚™ã—ã¾ã™
        """
        print("=== é¢æ¥éŸ³å£°åˆ†æãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆæœŸåŒ–ä¸­ ===")
        
        # æ—¥æœ¬èªå½¢æ…‹ç´ è§£æå™¨ã®åˆæœŸåŒ–
        print("æ—¥æœ¬èªè§£æãƒ„ãƒ¼ãƒ«ã‚’æº–å‚™ä¸­...")
        self.tokenizer = Tokenizer()
        
        # è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆèºŠèº‡è¡¨ç¾ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ãªã©ï¼‰ã®èª­ã¿è¾¼ã¿
        print("è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        self.patterns = self._load_patterns()
        
        # Whisperãƒ¢ãƒ‡ãƒ«ã¯ä½¿ç”¨æ™‚ã«é…å»¶èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ï¼‰
        self.whisper_model = None
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        OUTPUT_DIR.mkdir(exist_ok=True)
        CACHE_DIR.mkdir(exist_ok=True)
        
        print("åˆæœŸåŒ–å®Œäº†ï¼\n")

    def _load_patterns(self) -> dict:
        """
        è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Returns:
            dict: èºŠèº‡è¡¨ç¾ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³è¾æ›¸
            
        Notes:
            speech_patterns.yamlãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ä»¥ä¸‹ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™ï¼š
            - hesitation: èºŠèº‡è¡¨ç¾ï¼ˆãˆãƒ¼ã¨ã€ã‚ã®ãƒ¼ ãªã©ï¼‰
            - persistence: ç²˜ã‚Šå¼·ã•è¡¨ç¾ï¼ˆç¶™ç¶šã—ãŸã€é ‘å¼µã£ãŸ ãªã©ï¼‰
            - positive: ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆæˆåŠŸã€é”æˆ ãªã©ï¼‰
            - negative: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¤±æ•—ã€å›°é›£ ãªã©ï¼‰
            - conjunctions: æ¥ç¶šè©ï¼ˆã—ã‹ã—ã€ãã—ã¦ ãªã©ï¼‰
            - person: ä¸€äººç§°ãƒ»ä¸‰äººç§°è¡¨ç¾
            - thresholds: å„è©•ä¾¡æŒ‡æ¨™ã®é–¾å€¤
        """
        if not PATTERNS_YAML.exists():
            print(f"ã‚¨ãƒ©ãƒ¼: è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ â†’ {PATTERNS_YAML}")
            print("database/speech_patterns.yamlãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚")
            sys.exit(1)
        
        try:
            with open(PATTERNS_YAML, 'r', encoding='utf-8') as f:
                patterns = yaml.safe_load(f)
            print(f"   âœ“ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(patterns.get('patterns', {}))}ç¨®é¡")
            return patterns
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            sys.exit(1)

    def _load_whisper_model(self):
        """
        WhisperéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’é…å»¶èª­ã¿è¾¼ã¿
        
        Notes:
            åˆå›å®Ÿè¡Œæ™‚ã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚
            æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆç´„1-2åˆ†ï¼‰ã€‚2å›ç›®ä»¥é™ã¯é«˜é€Ÿã«èµ·å‹•ã—ã¾ã™ã€‚
        """
        if self.whisper_model is None:
            print("   WhisperéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            print("   ï¼ˆåˆå›å®Ÿè¡Œæ™‚ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰")
            try:
                # mediumãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ï¼‰
                self.whisper_model = whisper.load_model("medium")
                print("   âœ“ Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            except Exception as e:
                print(f"   ã‚¨ãƒ©ãƒ¼: Whisperãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                sys.exit(1)

    def convert_audio_to_wav(self, src: Path, dst: Path):
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’WAVå½¢å¼ã«å¤‰æ›
        
        Args:
            src (Path): å¤‰æ›å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆm4aå½¢å¼ï¼‰
            dst (Path): å¤‰æ›å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆwavå½¢å¼ï¼‰
            
        Notes:
            Whisperã‚„SpeechBrainã¯WAVå½¢å¼ã‚’æ¨å¥¨ã™ã‚‹ãŸã‚ã€
            m4aãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«WAVå½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚
        """
        print("â‘  éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼å¤‰æ›")
        start_time = time.time()
        
        try:
            # pydubã‚’ä½¿ç”¨ã—ã¦m4aâ†’wavå¤‰æ›
            audio = AudioSegment.from_file(src)
            audio.export(dst, format="wav")
            
            duration = time.time() - start_time
            print(f"   âœ“ å¤‰æ›å®Œäº† ({duration:.1f}ç§’)")
            print(f"   éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«é•·: {len(audio)/1000:.1f}ç§’\n")
            
        except Exception as e:
            print(f"   ã‚¨ãƒ©ãƒ¼: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("   m4aãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ã„å½¢å¼ã‹ã”ç¢ºèªãã ã•ã„ã€‚")
            sys.exit(1)

    def load_cached_results(self):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸåˆ†æçµæœã‚’èª­ã¿è¾¼ã‚€
        
        Returns:
            list or None: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã€ã¾ãŸã¯ Noneï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
            
        Notes:
            ä¸€åº¦åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã¨çµæœãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã€
            æ¬¡å›å®Ÿè¡Œæ™‚ã¯é«˜é€Ÿã«çµæœã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚
            æ–°ã—ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã§åˆ†æã—ãŸã„å ´åˆã¯ã€
            cacheãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚
        """
        cache_file = CACHE_DIR / "analysis_cache.csv"
        
        if not cache_file.exists():
            return None
            
        print("â‘¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸåˆ†æçµæœã‚’èª­ã¿è¾¼ã¿")
        start_time = time.time()
        
        try:
            df = pd.read_csv(cache_file)
            segments = []
            
            # CSVã‹ã‚‰ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
            for _, row in df.iterrows():
                segment = {
                    "start": row["start"],
                    "end": row["end"],
                    "text": row["text"],
                    "is_interviewer": row["is_interviewer"],
                    "metrics": {
                        "tokens": row["tokens"],
                        "hesitation": row["hesitation"],
                        "persistence": row["persistence"],
                        "conjunctions": row["conjunctions"],
                        "first_person": row["first_person"],
                        "third_person": row["third_person"],
                        "positive": row.get("positive", 0),
                        "negative": row.get("negative", 0)
                    }
                }
                segments.append(segment)
            
            duration = time.time() - start_time
            print(f"   âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿å®Œäº† ({duration:.1f}ç§’)")
            print(f"   èª­ã¿è¾¼ã¿ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments)}å€‹")
            print("   â€» æ–°ã—ã„éŸ³å£°ã§åˆ†æã—ãŸã„å ´åˆã¯ cacheãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„\n")
            
            return segments
            
        except Exception as e:
            print(f"   è­¦å‘Š: ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("   æ–°è¦ã«åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™\n")
            return None

    def transcribe_audio_with_whisper(self, audio_path: Path) -> list:
        """
        Whisperã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—
        
        Args:
            audio_path (Path): éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆwavå½¢å¼ï¼‰
            
        Returns:
            list: æ–‡å­—èµ·ã“ã—çµæœã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            
        Notes:
            Whisperã¯é«˜ç²¾åº¦ãªéŸ³å£°èªè­˜AIã§ã™ã€‚
            æ—¥æœ¬èªã«å¯¾å¿œã—ã¦ãŠã‚Šã€å¥èª­ç‚¹ã‚‚è‡ªå‹•ã§æŒ¿å…¥ã•ã‚Œã¾ã™ã€‚
        """
        print("â‘¡ éŸ³å£°æ–‡å­—èµ·ã“ã—ï¼ˆWhisper AIä½¿ç”¨ï¼‰")
        start_time = time.time()
        
        # Whisperãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        self._load_whisper_model()
        
        try:
            # Whisperã§éŸ³å£°èªè­˜å®Ÿè¡Œ
            result = self.whisper_model.transcribe(
                str(audio_path),
                language="ja",  # æ—¥æœ¬èªæŒ‡å®š
                task="transcribe",  # æ–‡å­—èµ·ã“ã—ã‚¿ã‚¹ã‚¯
                verbose=False  # è©³ç´°ãƒ­ã‚°ç„¡åŠ¹åŒ–
            )
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’æ•´ç†
            segments = []
            for segment in result["segments"]:
                segments.append({
                    "start": segment["start"],  # é–‹å§‹æ™‚åˆ»ï¼ˆç§’ï¼‰
                    "end": segment["end"],      # çµ‚äº†æ™‚åˆ»ï¼ˆç§’ï¼‰
                    "text": segment["text"].strip(),  # æ–‡å­—èµ·ã“ã—çµæœ
                    "speaker": -1  # è©±è€…ã¯å¾Œã§è­˜åˆ¥
                })
            
            duration = time.time() - start_time
            print(f"   âœ“ æ–‡å­—èµ·ã“ã—å®Œäº† ({duration:.1f}ç§’)")
            print(f"   èªè­˜ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments)}å€‹")
            print(f"   ç·æ–‡å­—æ•°: {sum(len(s['text']) for s in segments)}æ–‡å­—\n")
            
            return segments
            
        except Exception as e:
            print(f"   ã‚¨ãƒ©ãƒ¼: éŸ³å£°èªè­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            sys.exit(1)

    def identify_speakers(self, wav_path: Path, segments: list) -> list:
        """
        éŸ³å£°ã‹ã‚‰è©±è€…ã‚’è­˜åˆ¥ï¼ˆé¢æ¥å®˜ vs é¢æ¥è€…ï¼‰
        
        Args:
            wav_path (Path): éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            segments (list): æ–‡å­—èµ·ã“ã—ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            
        Returns:
            list: è©±è€…æƒ…å ±ãŒè¿½åŠ ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            
        Notes:
            SpeechBrainã®è©±è€…èªè­˜AIã‚’ä½¿ç”¨ã—ã€éŸ³å£°ã®ç‰¹å¾´ã‹ã‚‰
            2äººã®è©±è€…ã‚’è‡ªå‹•è­˜åˆ¥ã—ã¾ã™ã€‚ç™ºè©±å›æ•°ãŒå°‘ãªã„æ–¹ã‚’
            é¢æ¥å®˜ã¨åˆ¤å®šã—ã¾ã™ã€‚
        """
        print("â‘¢ è©±è€…è­˜åˆ¥ï¼ˆé¢æ¥å®˜ vs é¢æ¥è€…ã®åŒºåˆ¥ï¼‰")
        start_time = time.time()
        
        try:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            waveform, sample_rate = torchaudio.load(str(wav_path))
            
            # 16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆSpeechBrainã®æ¨å¥¨è¨­å®šï¼‰
            if sample_rate != 16000:
                waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)
            
            # ãƒ¢ãƒãƒ©ãƒ«åŒ–ï¼ˆè¤‡æ•°ãƒãƒ£ãƒ³ãƒãƒ«ã®å ´åˆï¼‰
            waveform = waveform.mean(dim=0)
            
            # SpeechBrainè©±è€…èªè­˜ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            print("   SpeechBrainè©±è€…èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            speaker_recognition = SpeakerRecognition.from_hparams(
                source="speechbrain/spkrec-ecapa-voxceleb",
                savedir="models/pretrained",
                run_opts={"device": "cpu"}  # CPUã‚’ä½¿ç”¨
            )
            
            # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®éŸ³å£°ç‰¹å¾´ã‚’æŠ½å‡º
            embeddings = []
            print("   å„ç™ºè©±ã®éŸ³å£°ç‰¹å¾´ã‚’æŠ½å‡ºä¸­...")
            
            for segment in tqdm(segments, desc="   éŸ³å£°ç‰¹å¾´æŠ½å‡º"):
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®éŸ³å£°éƒ¨åˆ†ã‚’åˆ‡ã‚Šå‡ºã—
                start_sample = int(segment["start"] * 16000)
                end_sample = int(segment["end"] * 16000)
                segment_audio = waveform[start_sample:end_sample].unsqueeze(0)
                
                # éŸ³å£°ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
                if len(segment_audio[0]) > 0:  # ç©ºã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                    embedding = speaker_recognition.encode_batch(segment_audio)
                    embeddings.append(embedding.squeeze().cpu().numpy())
                else:
                    # ç©ºã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«
                    embeddings.append(np.zeros(192))  # ECAãƒ‘ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›æ¬¡å…ƒ
            
            # è©±è€…ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆ2äººã®è©±è€…ã«åˆ†é¡ï¼‰
            print("   è©±è€…ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¸­...")
            embeddings_array = np.stack(embeddings)
            clustering = AgglomerativeClustering(n_clusters=2)
            speaker_labels = clustering.fit_predict(embeddings_array)
            
            # ç™ºè©±å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦é¢æ¥å®˜ã‚’åˆ¤å®š
            speaker_counts = Counter(speaker_labels)
            # ç™ºè©±å›æ•°ãŒå°‘ãªã„æ–¹ã‚’é¢æ¥å®˜ã¨åˆ¤å®š
            interviewer_label = speaker_counts.most_common()[-1][0]
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«è©±è€…æƒ…å ±ã‚’è¿½åŠ 
            for segment, label in zip(segments, speaker_labels):
                segment["is_interviewer"] = (label == interviewer_label)
            
            # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
            interviewer_count = sum(1 for s in segments if s["is_interviewer"])
            interviewee_count = len(segments) - interviewer_count
            
            duration = time.time() - start_time
            print(f"   âœ“ è©±è€…è­˜åˆ¥å®Œäº† ({duration:.1f}ç§’)")
            print(f"   é¢æ¥å®˜ç™ºè©±: {interviewer_count}å›")
            print(f"   é¢æ¥è€…ç™ºè©±: {interviewee_count}å›\n")
            
            return segments
            
        except Exception as e:
            print(f"   ã‚¨ãƒ©ãƒ¼: è©±è€…è­˜åˆ¥ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("   å…¨ã¦ã®ç™ºè©±ã‚’é¢æ¥è€…ã¨ã—ã¦å‡¦ç†ã—ã¾ã™")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…¨ã¦é¢æ¥è€…ã¨ã—ã¦æ‰±ã†
            for segment in segments:
                segment["is_interviewer"] = False
            
            return segments 

    def analyze_speech_patterns(self, text: str) -> dict:
        """
        ç™ºè©±ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        
        Args:
            text (str): åˆ†æå¯¾è±¡ã®ç™ºè©±ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            dict: å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾å›æ•°
            
        Notes:
            ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¾ã™ï¼š
            - èºŠèº‡è¡¨ç¾ï¼ˆãˆãƒ¼ã¨ã€ã‚ã®ãƒ¼ ãªã©ï¼‰
            - ç²˜ã‚Šå¼·ã•è¡¨ç¾ï¼ˆé ‘å¼µã£ãŸã€ç¶™ç¶šã—ãŸ ãªã©ï¼‰
            - ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆæˆåŠŸã€é”æˆ ãªã©ï¼‰
            - ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¤±æ•—ã€å›°é›£ ãªã©ï¼‰
            - æ¥ç¶šè©ï¼ˆã—ã‹ã—ã€ãã—ã¦ ãªã©ï¼‰
            - ä¸€äººç§°ãƒ»ä¸‰äººç§°è¡¨ç¾ï¼ˆç§ã€å½¼ ãªã©ï¼‰
        """
        # æ—¥æœ¬èªå½¢æ…‹ç´ è§£æã§å˜èªã«åˆ†å‰²
        tokens = [token.surface for token in self.tokenizer.tokenize(text)]
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        pattern_counts = {
            "tokens": len(tokens),  # ç·å˜èªæ•°
            
            # èºŠèº‡è¡¨ç¾ã®ã‚«ã‚¦ãƒ³ãƒˆ
            "hesitation": sum(
                hesitation_word in text 
                for hesitation_word in self.patterns["patterns"]["hesitation"]
            ),
            
            # ç²˜ã‚Šå¼·ã•è¡¨ç¾ã®ã‚«ã‚¦ãƒ³ãƒˆ
            "persistence": sum(
                persistence_word in text 
                for persistence_word in self.patterns["patterns"]["persistence"]
            ),
            
            # æ¥ç¶šè©ã®ã‚«ã‚¦ãƒ³ãƒˆ
            "conjunctions": sum(
                conjunction in text 
                for conjunction in self.patterns["patterns"]["conjunctions"]
            ),
            
            # ä¸€äººç§°è¡¨ç¾ã®ã‚«ã‚¦ãƒ³ãƒˆ
            "first_person": sum(
                first_person in text 
                for first_person in self.patterns["patterns"]["person"]["first_person"]
            ),
            
            # ä¸‰äººç§°è¡¨ç¾ã®ã‚«ã‚¦ãƒ³ãƒˆ
            "third_person": sum(
                third_person in text 
                for third_person in self.patterns["patterns"]["person"]["third_person"]
            ),
            
            # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã®ã‚«ã‚¦ãƒ³ãƒˆ
            "positive": sum(
                positive_word in text 
                for positive_word in self.patterns["patterns"]["positive"]
            ),
            
            # ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã®ã‚«ã‚¦ãƒ³ãƒˆ
            "negative": sum(
                negative_word in text 
                for negative_word in self.patterns["patterns"]["negative"]
            )
        }
        
        return pattern_counts

    def calculate_evaluation_metrics(self, segments: list) -> dict:
        """
        é¢æ¥è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
        
        Args:
            segments (list): åˆ†ææ¸ˆã¿ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            
        Returns:
            dict: 7ã¤ã®è©•ä¾¡æŒ‡æ¨™ã¨ãã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
            
        Notes:
            è¨ˆç®—ã•ã‚Œã‚‹è©•ä¾¡æŒ‡æ¨™ï¼š
            1. ãƒã‚¸ãƒ†ã‚£ãƒ– vs ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã®æ¯”ç‡
            2. èºŠèº‡ç‡ï¼ˆãƒ•ã‚£ãƒ©ãƒ¼é »åº¦ï¼‰
            3. ç²˜ã‚Šå¼·ã•ã®è¡¨ç¾é »åº¦
            4. å¹³å‡å¿œç­”é€Ÿåº¦
            5. æ¥ç¶šè©ä½¿ç”¨é »åº¦
            6. ç™ºè©±é€Ÿåº¦ï¼ˆWPMï¼‰
            7. ä¸€äººç§°/ä¸‰äººç§°æ¯”ç‡
        """
        print("â‘£ è©•ä¾¡æŒ‡æ¨™è¨ˆç®—")
        start_time = time.time()
        
        # é¢æ¥è€…ã®ç™ºè©±ã®ã¿ã‚’æŠ½å‡ºï¼ˆé¢æ¥å®˜ã®ç™ºè©±ã¯é™¤å¤–ï¼‰
        interviewee_segments = [s for s in segments if not s["is_interviewer"]]
        
        if not interviewee_segments:
            print("   è­¦å‘Š: é¢æ¥è€…ã®ç™ºè©±ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return {}
        
        # åŸºæœ¬çµ±è¨ˆã®é›†è¨ˆ
        total_tokens = sum(s["metrics"]["tokens"] for s in interviewee_segments)
        total_duration = sum(s["end"] - s["start"] for s in interviewee_segments)
        total_hesitation = sum(s["metrics"]["hesitation"] for s in interviewee_segments)
        total_persistence = sum(s["metrics"]["persistence"] for s in interviewee_segments)
        total_conjunctions = sum(s["metrics"]["conjunctions"] for s in interviewee_segments)
        total_first_person = sum(s["metrics"]["first_person"] for s in interviewee_segments)
        total_third_person = sum(s["metrics"]["third_person"] for s in interviewee_segments)
        total_positive = sum(s["metrics"]["positive"] for s in interviewee_segments)
        total_negative = sum(s["metrics"]["negative"] for s in interviewee_segments)
        
        # å¿œç­”é€Ÿåº¦ã®è¨ˆç®—ï¼ˆé¢æ¥å®˜ã®è³ªå•çµ‚äº†ã‹ã‚‰é¢æ¥è€…ã®å›ç­”é–‹å§‹ã¾ã§ï¼‰
        response_times = []
        for i, segment in enumerate(segments[:-1]):
            if segment["is_interviewer"] and not segments[i+1]["is_interviewer"]:
                # é¢æ¥å®˜â†’é¢æ¥è€…ã®åˆ‡ã‚Šæ›¿ã‚ã‚Šæ™‚ã®å¿œç­”æ™‚é–“
                response_time = segments[i+1]["start"] - segment["end"]
                if response_time >= 0:  # è² ã®å€¤ã¯é™¤å¤–
                    response_times.append(response_time)
        
        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        metrics = {
            # === 7ã¤ã®ä¸»è¦è©•ä¾¡æŒ‡æ¨™ ===
            
            # 1. ãƒã‚¸ãƒ†ã‚£ãƒ– vs ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã®æ¯”ç‡
            "positive_negative_ratio": (
                total_positive / total_negative if total_negative > 0 else float('inf')
            ),
            
            # 2. èºŠèº‡ç‡ï¼ˆç·å˜èªæ•°ã«å¯¾ã™ã‚‹èºŠèº‡è¡¨ç¾ã®å‰²åˆï¼‰
            "hesitation_rate": (
                total_hesitation / total_tokens if total_tokens > 0 else 0
            ),
            
            # 3. ç²˜ã‚Šå¼·ã•ã®è¡¨ç¾é »åº¦ï¼ˆ1000å˜èªã‚ãŸã‚Šï¼‰
            "persistence_rate": (
                (total_persistence * 1000) / total_tokens if total_tokens > 0 else 0
            ),
            
            # 4. å¹³å‡å¿œç­”é€Ÿåº¦ï¼ˆç§’ï¼‰
            "avg_response_time": (
                np.mean(response_times) if response_times else 0
            ),
            
            # 5. æ¥ç¶šè©ä½¿ç”¨é »åº¦ï¼ˆ1000å˜èªã‚ãŸã‚Šï¼‰
            "conjunction_rate": (
                (total_conjunctions * 1000) / total_tokens if total_tokens > 0 else 0
            ),
            
            # 6. ç™ºè©±é€Ÿåº¦ï¼ˆWPM: Words Per Minuteï¼‰
            "speech_rate_wpm": (
                (total_tokens * 60) / total_duration if total_duration > 0 else 0
            ),
            
            # 7. ä¸€äººç§°/ä¸‰äººç§°æ¯”ç‡
            "first_third_ratio": (
                total_first_person / total_third_person if total_third_person > 0 else float('inf')
            ),
            
            # === è©³ç´°ãƒ‡ãƒ¼ã‚¿ ===
            "total_segments": len(interviewee_segments),
            "total_duration_minutes": total_duration / 60,
            "total_tokens": total_tokens,
            "total_hesitation": total_hesitation,
            "total_persistence": total_persistence,
            "total_conjunctions": total_conjunctions,
            "total_positive": total_positive,
            "total_negative": total_negative,
            "total_first_person": total_first_person,
            "total_third_person": total_third_person,
            "response_times": response_times
        }
        
        # è©•ä¾¡åŸºæº–ã¨ã®æ¯”è¼ƒï¼ˆè‰¯å¥½/æ™®é€š/è¦æ”¹å–„ã®åˆ¤å®šï¼‰
        thresholds = self.patterns["thresholds"]
        
        # å„æŒ‡æ¨™ã®è©•ä¾¡ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š
        metrics.update({
            "positive_negative_evaluation": (
                "è‰¯å¥½" if metrics["positive_negative_ratio"] >= thresholds["positive_negative_ratio"]["good"]
                else "æ™®é€š" if metrics["positive_negative_ratio"] >= thresholds["positive_negative_ratio"]["acceptable"]
                else "è¦æ”¹å–„"
            ),
            
            "hesitation_evaluation": (
                "è‰¯å¥½" if metrics["hesitation_rate"] <= thresholds["hesitation_rate"]["good"]
                else "æ™®é€š" if metrics["hesitation_rate"] <= thresholds["hesitation_rate"]["acceptable"]
                else "è¦æ”¹å–„"
            ),
            
            "response_time_evaluation": (
                "è‰¯å¥½" if metrics["avg_response_time"] <= thresholds["response_latency"]["good"]
                else "æ™®é€š" if metrics["avg_response_time"] <= thresholds["response_latency"]["acceptable"]
                else "è¦æ”¹å–„"
            ),
            
            "speech_rate_evaluation": (
                "é…ã„" if metrics["speech_rate_wpm"] < thresholds["speech_rate"]["slow"]
                else "é©æ­£" if metrics["speech_rate_wpm"] < thresholds["speech_rate"]["fast"]
                else "é€Ÿã„"
            )
        })
        
        duration = time.time() - start_time
        print(f"   âœ“ è©•ä¾¡æŒ‡æ¨™è¨ˆç®—å®Œäº† ({duration:.1f}ç§’)\n")
        
        return metrics 

    def create_visualization_charts(self, segments: list, metrics: dict):
        """
        åˆ†æçµæœã®å¯è¦–åŒ–ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆ
        
        Args:
            segments (list): åˆ†ææ¸ˆã¿ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            metrics (dict): è¨ˆç®—æ¸ˆã¿è©•ä¾¡æŒ‡æ¨™
            
        Notes:
            ä»¥ä¸‹ã®ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ï¼š
            1. ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆç·åˆè©•ä¾¡ï¼‰
            2. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆè©³ç´°ï¼‰
            3. æ™‚ç³»åˆ—ãƒãƒ£ãƒ¼ãƒˆ
            4. å¿œç­”é€Ÿåº¦åˆ†æ
        """
        print("â‘¤ å¯è¦–åŒ–ãƒãƒ£ãƒ¼ãƒˆä½œæˆ")
        start_time = time.time()
        
        # ãƒãƒ£ãƒ¼ãƒˆä½œæˆ
        self._create_radar_chart(metrics)
        self._create_pattern_analysis_chart(metrics)
        self._create_timeline_chart(segments, metrics)
        self._create_response_time_chart(metrics)
        
        duration = time.time() - start_time
        print(f"   âœ“ ãƒãƒ£ãƒ¼ãƒˆä½œæˆå®Œäº† ({duration:.1f}ç§’)\n")

    def _create_radar_chart(self, metrics: dict):
        """è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        # è©•ä¾¡æŒ‡æ¨™ã®æº–å‚™
        categories = [
            'ãƒã‚¸ãƒ†ã‚£ãƒ–/\nãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”',
            'èºŠèº‡ç‡\n(ä½ã„ã»ã©è‰¯ã„)',
            'ç²˜ã‚Šå¼·ã•è¡¨ç¾',
            'å¿œç­”é€Ÿåº¦\n(é€Ÿã„ã»ã©è‰¯ã„)',
            'æ¥ç¶šè©ä½¿ç”¨',
            'ç™ºè©±é€Ÿåº¦',
            'ä¸€äººç§°/\nä¸‰äººç§°æ¯”'
        ]
        
        # å„æŒ‡æ¨™ã‚’0-10ã®ç¯„å›²ã«æ­£è¦åŒ–
        values = [
            min(metrics.get("positive_negative_ratio", 0), 10),  # ä¸Šé™10
            max(0, 10 - metrics.get("hesitation_rate", 0) * 100),  # èºŠèº‡ç‡ã¯åè»¢
            min(metrics.get("persistence_rate", 0) / 10, 10),  # 10ã§å‰²ã£ã¦æ­£è¦åŒ–
            max(0, 10 - metrics.get("avg_response_time", 0)),  # å¿œç­”é€Ÿåº¦ã¯åè»¢
            min(metrics.get("conjunction_rate", 0) / 10, 10),  # 10ã§å‰²ã£ã¦æ­£è¦åŒ–
            min(metrics.get("speech_rate_wpm", 0) / 20, 10),  # 20ã§å‰²ã£ã¦æ­£è¦åŒ–
            min(metrics.get("first_third_ratio", 0), 10)  # ä¸Šé™10
        ]
        
        # è§’åº¦ã®è¨ˆç®—
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values += values[:1]  # é–‰ã˜ã‚‹ãŸã‚ã«æœ€åˆã®å€¤ã‚’æœ«å°¾ã«è¿½åŠ 
        angles += angles[:1]
        
        # ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(angles, values, 'o-', linewidth=2, color='#FF6B6B')
        ax.fill(angles, values, alpha=0.25, color='#FF6B6B')
        
        # ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=12)
        ax.set_ylim(0, 10)
        ax.set_yticks([2, 4, 6, 8, 10])
        ax.set_yticklabels(['2', '4', '6', '8', '10'], fontsize=10)
        ax.grid(True)
        
        plt.title('é¢æ¥è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ', size=16, fontweight='bold', pad=20)
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / 'è©•ä¾¡æŒ‡æ¨™_ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_pattern_analysis_chart(self, metrics: dict):
        """ç™ºè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ãƒã‚¸ãƒ†ã‚£ãƒ– vs ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰
        pos_neg_data = [metrics.get("total_positive", 0), metrics.get("total_negative", 0)]
        colors1 = ['#4CAF50', '#F44336']
        ax1.bar(['ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰', 'ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰'], pos_neg_data, color=colors1)
        ax1.set_title('ãƒã‚¸ãƒ†ã‚£ãƒ– vs ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ä½¿ç”¨æ•°')
        ax1.set_ylabel('å‡ºç¾å›æ•°')
        for i, v in enumerate(pos_neg_data):
            ax1.text(i, v + max(pos_neg_data) * 0.01, str(v), ha='center', fontweight='bold')
        
        # 2. èºŠèº‡ãƒ»ç²˜ã‚Šå¼·ã•è¡¨ç¾
        hesit_persist_data = [
            metrics.get("total_hesitation", 0),
            metrics.get("total_persistence", 0)
        ]
        colors2 = ['#FF9800', '#2196F3']
        ax2.bar(['èºŠèº‡è¡¨ç¾', 'ç²˜ã‚Šå¼·ã•è¡¨ç¾'], hesit_persist_data, color=colors2)
        ax2.set_title('èºŠèº‡è¡¨ç¾ vs ç²˜ã‚Šå¼·ã•è¡¨ç¾')
        ax2.set_ylabel('å‡ºç¾å›æ•°')
        for i, v in enumerate(hesit_persist_data):
            ax2.text(i, v + max(hesit_persist_data) * 0.01, str(v), ha='center', fontweight='bold')
        
        # 3. ä¸€äººç§° vs ä¸‰äººç§°
        person_data = [
            metrics.get("total_first_person", 0),
            metrics.get("total_third_person", 0)
        ]
        colors3 = ['#9C27B0', '#607D8B']
        ax3.bar(['ä¸€äººç§°è¡¨ç¾', 'ä¸‰äººç§°è¡¨ç¾'], person_data, color=colors3)
        ax3.set_title('ä¸€äººç§° vs ä¸‰äººç§°è¡¨ç¾ä½¿ç”¨æ•°')
        ax3.set_ylabel('å‡ºç¾å›æ•°')
        for i, v in enumerate(person_data):
            ax3.text(i, v + max(person_data) * 0.01, str(v), ha='center', fontweight='bold')
        
        # 4. ç™ºè©±çµ±è¨ˆ
        speech_stats = [
            metrics.get("total_tokens", 0),
            metrics.get("total_conjunctions", 0),
            metrics.get("total_segments", 0)
        ]
        colors4 = ['#795548', '#FF5722', '#009688']
        bars = ax4.bar(['ç·å˜èªæ•°', 'æ¥ç¶šè©æ•°', 'ç™ºè©±å›æ•°'], speech_stats, color=colors4)
        ax4.set_title('ç™ºè©±çµ±è¨ˆ')
        ax4.set_ylabel('æ•°é‡')
        for i, v in enumerate(speech_stats):
            ax4.text(i, v + max(speech_stats) * 0.01, str(v), ha='center', fontweight='bold')
        
        plt.suptitle('ç™ºè©±ãƒ‘ã‚¿ãƒ¼ãƒ³è©³ç´°åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / 'ç™ºè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_timeline_chart(self, segments: list, metrics: dict):
        """æ™‚ç³»åˆ—ç™ºè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä½œæˆ"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
        
        # é¢æ¥è€…ã®ç™ºè©±ã®ã¿æŠ½å‡º
        interviewee_segments = [s for s in segments if not s["is_interviewer"]]
        
        if not interviewee_segments:
            return
        
        times = [(s["start"] + s["end"]) / 2 for s in interviewee_segments]  # ç™ºè©±ä¸­å¤®æ™‚åˆ»
        hesitations = [s["metrics"]["hesitation"] for s in interviewee_segments]
        positives = [s["metrics"]["positive"] for s in interviewee_segments]
        negatives = [s["metrics"]["negative"] for s in interviewee_segments]
        
        # 1. èºŠèº‡è¡¨ç¾ã®æ™‚ç³»åˆ—å¤‰åŒ–
        ax1.plot(times, hesitations, 'o-', color='#FF6B6B', linewidth=2, markersize=6)
        ax1.set_title('ç™ºè©±æ™‚é–“ã«ãŠã‘ã‚‹èºŠèº‡è¡¨ç¾ã®å¤‰åŒ–')
        ax1.set_xlabel('æ™‚é–“ (ç§’)')
        ax1.set_ylabel('èºŠèº‡è¡¨ç¾æ•°')
        ax1.grid(True, alpha=0.3)
        ax1.fill_between(times, hesitations, alpha=0.3, color='#FF6B6B')
        
        # 2. ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã®æ™‚ç³»åˆ—å¤‰åŒ–
        ax2.plot(times, positives, 'o-', color='#4CAF50', linewidth=2, 
                markersize=6, label='ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰')
        ax2.plot(times, negatives, 'o-', color='#F44336', linewidth=2, 
                markersize=6, label='ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰')
        ax2.set_title('ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã®æ™‚ç³»åˆ—å¤‰åŒ–')
        ax2.set_xlabel('æ™‚é–“ (ç§’)')
        ax2.set_ylabel('ãƒ¯ãƒ¼ãƒ‰æ•°')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / 'æ™‚ç³»åˆ—ç™ºè©±ãƒ‘ã‚¿ãƒ¼ãƒ³.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_response_time_chart(self, metrics: dict):
        """å¿œç­”é€Ÿåº¦åˆ†å¸ƒãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        response_times = metrics.get("response_times", [])
        
        if not response_times:
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 1. å¿œç­”é€Ÿåº¦ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        ax1.hist(response_times, bins=min(len(response_times), 10), 
                color='#3F51B5', alpha=0.7, edgecolor='black')
        ax1.axvline(np.mean(response_times), color='red', linestyle='--', 
                   linewidth=2, label=f'å¹³å‡: {np.mean(response_times):.1f}ç§’')
        ax1.set_title('å¿œç­”é€Ÿåº¦åˆ†å¸ƒ')
        ax1.set_xlabel('å¿œç­”æ™‚é–“ (ç§’)')
        ax1.set_ylabel('é »åº¦')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å¿œç­”é€Ÿåº¦ã®æ™‚ç³»åˆ—å¤‰åŒ–
        ax2.plot(range(1, len(response_times) + 1), response_times, 
                'o-', color='#3F51B5', linewidth=2, markersize=8)
        ax2.axhline(np.mean(response_times), color='red', linestyle='--', 
                   linewidth=2, label=f'å¹³å‡: {np.mean(response_times):.1f}ç§’')
        ax2.set_title('å¿œç­”é€Ÿåº¦ã®æ™‚ç³»åˆ—å¤‰åŒ–')
        ax2.set_xlabel('è³ªå•ç•ªå·')
        ax2.set_ylabel('å¿œç­”æ™‚é–“ (ç§’)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / 'å¿œç­”é€Ÿåº¦åˆ†æ.png', dpi=300, bbox_inches='tight')
        plt.close()

    def save_results_to_files(self, segments: list, metrics: dict):
        """
        åˆ†æçµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            segments (list): åˆ†ææ¸ˆã¿ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            metrics (dict): è¨ˆç®—æ¸ˆã¿è©•ä¾¡æŒ‡æ¨™
            
        Notes:
            ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã™ï¼š
            - cache/analysis_cache.csv: è©³ç´°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰
            - output/é¢æ¥åˆ†æçµæœ_æ¦‚è¦.csv: è©•ä¾¡æŒ‡æ¨™ã‚µãƒãƒªãƒ¼
            - output/é¢æ¥åˆ†æçµæœ_è©³ç´°.csv: å…¨ç™ºè©±ãƒ‡ãƒ¼ã‚¿
        """
        print("â‘¥ åˆ†æçµæœä¿å­˜")
        start_time = time.time()
        
        # === ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ ===
        CACHE_DIR.mkdir(exist_ok=True)
        cache_data = []
        
        for segment in segments:
            row = {
                "start": segment["start"],
                "end": segment["end"],
                "text": segment["text"],
                "is_interviewer": segment["is_interviewer"],
                "tokens": segment["metrics"]["tokens"],
                "hesitation": segment["metrics"]["hesitation"],
                "persistence": segment["metrics"]["persistence"],
                "conjunctions": segment["metrics"]["conjunctions"],
                "first_person": segment["metrics"]["first_person"],
                "third_person": segment["metrics"]["third_person"],
                "positive": segment["metrics"]["positive"],
                "negative": segment["metrics"]["negative"]
            }
            cache_data.append(row)
        
        pd.DataFrame(cache_data).to_csv(CACHE_DIR / "analysis_cache.csv", index=False)
        
        # === è©•ä¾¡æŒ‡æ¨™ã‚µãƒãƒªãƒ¼ã®ä¿å­˜ ===
        OUTPUT_DIR.mkdir(exist_ok=True)
        summary_data = [{
            "è©•ä¾¡é …ç›®": "ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡",
            "æ•°å€¤": f"{metrics.get('positive_negative_ratio', 0):.2f}",
            "è©•ä¾¡": metrics.get('positive_negative_evaluation', 'ä¸æ˜'),
            "è©³ç´°": f"ãƒã‚¸ãƒ†ã‚£ãƒ–:{metrics.get('total_positive', 0)}å€‹, ãƒã‚¬ãƒ†ã‚£ãƒ–:{metrics.get('total_negative', 0)}å€‹"
        }, {
            "è©•ä¾¡é …ç›®": "èºŠèº‡ç‡",
            "æ•°å€¤": f"{metrics.get('hesitation_rate', 0)*100:.1f}%",
            "è©•ä¾¡": metrics.get('hesitation_evaluation', 'ä¸æ˜'),
            "è©³ç´°": f"èºŠèº‡è¡¨ç¾:{metrics.get('total_hesitation', 0)}å€‹ / ç·å˜èªæ•°:{metrics.get('total_tokens', 0)}å€‹"
        }, {
            "è©•ä¾¡é …ç›®": "ç²˜ã‚Šå¼·ã•è¡¨ç¾é »åº¦",
            "æ•°å€¤": f"{metrics.get('persistence_rate', 0):.1f}/1000èª",
            "è©•ä¾¡": "è‰¯å¥½" if metrics.get('persistence_rate', 0) > 5 else "æ™®é€š" if metrics.get('persistence_rate', 0) > 2 else "è¦æ”¹å–„",
            "è©³ç´°": f"ç²˜ã‚Šå¼·ã•è¡¨ç¾:{metrics.get('total_persistence', 0)}å€‹"
        }, {
            "è©•ä¾¡é …ç›®": "å¹³å‡å¿œç­”é€Ÿåº¦",
            "æ•°å€¤": f"{metrics.get('avg_response_time', 0):.1f}ç§’",
            "è©•ä¾¡": metrics.get('response_time_evaluation', 'ä¸æ˜'),
            "è©³ç´°": f"å¿œç­”å›æ•°:{len(metrics.get('response_times', []))}å›"
        }, {
            "è©•ä¾¡é …ç›®": "æ¥ç¶šè©ä½¿ç”¨é »åº¦",
            "æ•°å€¤": f"{metrics.get('conjunction_rate', 0):.1f}/1000èª",
            "è©•ä¾¡": "è‰¯å¥½" if metrics.get('conjunction_rate', 0) > 10 else "æ™®é€š" if metrics.get('conjunction_rate', 0) > 5 else "è¦æ”¹å–„",
            "è©³ç´°": f"æ¥ç¶šè©:{metrics.get('total_conjunctions', 0)}å€‹"
        }, {
            "è©•ä¾¡é …ç›®": "ç™ºè©±é€Ÿåº¦",
            "æ•°å€¤": f"{metrics.get('speech_rate_wpm', 0):.1f} WPM",
            "è©•ä¾¡": metrics.get('speech_rate_evaluation', 'ä¸æ˜'),
            "è©³ç´°": f"ç·ç™ºè©±æ™‚é–“:{metrics.get('total_duration_minutes', 0):.1f}åˆ†"
        }, {
            "è©•ä¾¡é …ç›®": "ä¸€äººç§°/ä¸‰äººç§°æ¯”ç‡",
            "æ•°å€¤": f"{metrics.get('first_third_ratio', 0):.2f}",
            "è©•ä¾¡": "è‰¯å¥½" if 1 <= metrics.get('first_third_ratio', 0) <= 3 else "è¦æ³¨æ„",
            "è©³ç´°": f"ä¸€äººç§°:{metrics.get('total_first_person', 0)}å€‹, ä¸‰äººç§°:{metrics.get('total_third_person', 0)}å€‹"
        }]
        
        pd.DataFrame(summary_data).to_csv(OUTPUT_DIR / "é¢æ¥åˆ†æçµæœ_æ¦‚è¦.csv", index=False, encoding='utf-8-sig')
        
        # === è©³ç´°ç™ºè©±ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ ===
        detailed_data = []
        for i, segment in enumerate(segments):
            row = {
                "ç™ºè©±ç•ªå·": i + 1,
                "é–‹å§‹æ™‚åˆ»": f"{segment['start']:.1f}ç§’",
                "çµ‚äº†æ™‚åˆ»": f"{segment['end']:.1f}ç§’",
                "ç™ºè©±æ™‚é–“": f"{segment['end'] - segment['start']:.1f}ç§’",
                "è©±è€…": "é¢æ¥å®˜" if segment["is_interviewer"] else "é¢æ¥è€…",
                "ç™ºè©±å†…å®¹": segment["text"],
                "å˜èªæ•°": segment["metrics"]["tokens"],
                "èºŠèº‡è¡¨ç¾": segment["metrics"]["hesitation"],
                "ç²˜ã‚Šå¼·ã•è¡¨ç¾": segment["metrics"]["persistence"],
                "ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰": segment["metrics"]["positive"],
                "ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰": segment["metrics"]["negative"],
                "æ¥ç¶šè©": segment["metrics"]["conjunctions"],
                "ä¸€äººç§°": segment["metrics"]["first_person"],
                "ä¸‰äººç§°": segment["metrics"]["third_person"]
            }
            detailed_data.append(row)
        
        pd.DataFrame(detailed_data).to_csv(OUTPUT_DIR / "é¢æ¥åˆ†æçµæœ_è©³ç´°.csv", index=False, encoding='utf-8-sig')
        
        duration = time.time() - start_time
        print(f"   âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº† ({duration:.1f}ç§’)")
        print(f"   ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"     - {OUTPUT_DIR}/é¢æ¥åˆ†æçµæœ_æ¦‚è¦.csv")
        print(f"     - {OUTPUT_DIR}/é¢æ¥åˆ†æçµæœ_è©³ç´°.csv")
        print(f"     - {CACHE_DIR}/analysis_cache.csv (ã‚­ãƒ£ãƒƒã‚·ãƒ¥)\n")

    def display_results(self, metrics: dict, segments: list):
        """
        åˆ†æçµæœã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¡¨ç¤º
        
        Args:
            metrics (dict): è¨ˆç®—æ¸ˆã¿è©•ä¾¡æŒ‡æ¨™
            segments (list): åˆ†ææ¸ˆã¿ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        print("=" * 60)
        print("ğŸ“Š é¢æ¥éŸ³å£°åˆ†æçµæœ")
        print("=" * 60)
        
        # === åŸºæœ¬æƒ…å ± ===
        print("\nğŸ” åŸºæœ¬æƒ…å ±")
        print("-" * 40)
        print(f"ç·ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments)}å€‹")
        print(f"é¢æ¥è€…ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {metrics.get('total_segments', 0)}å€‹")
        print(f"ç·ç™ºè©±æ™‚é–“: {metrics.get('total_duration_minutes', 0):.1f}åˆ†")
        print(f"ç·å˜èªæ•°: {metrics.get('total_tokens', 0):,}å€‹")
        
        # === 7ã¤ã®è©•ä¾¡æŒ‡æ¨™ ===
        print("\nğŸ“ˆ é¢æ¥è©•ä¾¡æŒ‡æ¨™")
        print("-" * 40)
        
        print(f"1. ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”ç‡: {metrics.get('positive_negative_ratio', 0):.2f} "
              f"({metrics.get('positive_negative_evaluation', 'ä¸æ˜')})")
        print(f"   ğŸ“Š ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰: {metrics.get('total_positive', 0)}å€‹")
        print(f"   ğŸ“Š ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰: {metrics.get('total_negative', 0)}å€‹")
        
        print(f"\n2. èºŠèº‡ç‡: {metrics.get('hesitation_rate', 0)*100:.1f}% "
              f"({metrics.get('hesitation_evaluation', 'ä¸æ˜')})")
        print(f"   ğŸ“Š èºŠèº‡è¡¨ç¾: {metrics.get('total_hesitation', 0)}å€‹")
        
        print(f"\n3. ç²˜ã‚Šå¼·ã•è¡¨ç¾é »åº¦: {metrics.get('persistence_rate', 0):.1f}/1000èª")
        print(f"   ğŸ“Š ç²˜ã‚Šå¼·ã•è¡¨ç¾: {metrics.get('total_persistence', 0)}å€‹")
        
        print(f"\n4. å¹³å‡å¿œç­”é€Ÿåº¦: {metrics.get('avg_response_time', 0):.1f}ç§’ "
              f"({metrics.get('response_time_evaluation', 'ä¸æ˜')})")
        if metrics.get('response_times'):
            print(f"   ğŸ“Š å¿œç­”æ™‚é–“ç¯„å›²: {min(metrics['response_times']):.1f}ç§’ ï½ "
                  f"{max(metrics['response_times']):.1f}ç§’")
            print(f"   ğŸ“Š å¿œç­”å›æ•°: {len(metrics['response_times'])}å›")
        
        print(f"\n5. æ¥ç¶šè©ä½¿ç”¨é »åº¦: {metrics.get('conjunction_rate', 0):.1f}/1000èª")
        print(f"   ğŸ“Š æ¥ç¶šè©: {metrics.get('total_conjunctions', 0)}å€‹")
        
        print(f"\n6. ç™ºè©±é€Ÿåº¦: {metrics.get('speech_rate_wpm', 0):.1f} WPM "
              f"({metrics.get('speech_rate_evaluation', 'ä¸æ˜')})")
        
        print(f"\n7. ä¸€äººç§°/ä¸‰äººç§°æ¯”ç‡: {metrics.get('first_third_ratio', 0):.2f}")
        print(f"   ğŸ“Š ä¸€äººç§°: {metrics.get('total_first_person', 0)}å€‹")
        print(f"   ğŸ“Š ä¸‰äººç§°: {metrics.get('total_third_person', 0)}å€‹")
        
        # === ç™ºè©±ä¾‹ ===
        print("\nğŸ’¬ ç™ºè©±ä¾‹")
        print("-" * 40)
        
        # èºŠèº‡è¡¨ç¾ã‚’å«ã‚€ç™ºè©±ä¾‹
        hesitation_examples = [
            s for s in segments 
            if not s["is_interviewer"] and s["metrics"]["hesitation"] > 0
        ]
        
        if hesitation_examples:
            print("ğŸ¤” èºŠèº‡è¡¨ç¾ã‚’å«ã‚€ç™ºè©±ä¾‹:")
            for i, example in enumerate(hesitation_examples[:3]):
                text_preview = example["text"][:50] + "..." if len(example["text"]) > 50 else example["text"]
                print(f"   {i+1}. ã€Œ{text_preview}ã€(èºŠèº‡: {example['metrics']['hesitation']}å€‹)")
        else:
            print("ğŸ¤” èºŠèº‡è¡¨ç¾ã‚’å«ã‚€ç™ºè©±ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # ç²˜ã‚Šå¼·ã•è¡¨ç¾ã‚’å«ã‚€ç™ºè©±ä¾‹
        persistence_examples = [
            s for s in segments 
            if not s["is_interviewer"] and s["metrics"]["persistence"] > 0
        ]
        
        if persistence_examples:
            print("\nğŸ’ª ç²˜ã‚Šå¼·ã•è¡¨ç¾ã‚’å«ã‚€ç™ºè©±ä¾‹:")
            for i, example in enumerate(persistence_examples[:3]):
                text_preview = example["text"][:50] + "..." if len(example["text"]) > 50 else example["text"]
                print(f"   {i+1}. ã€Œ{text_preview}ã€(ç²˜ã‚Šå¼·ã•: {example['metrics']['persistence']}å€‹)")
        else:
            print("\nğŸ’ª ç²˜ã‚Šå¼·ã•è¡¨ç¾ã‚’å«ã‚€ç™ºè©±ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        print("\n" + "=" * 60)
        print("âœ… åˆ†æå®Œäº†ï¼è©³ç´°ãªçµæœã¯ä»¥ä¸‹ã§ç¢ºèªã§ãã¾ã™ï¼š")
        print(f"ğŸ“ CSVãƒ•ã‚¡ã‚¤ãƒ«: {OUTPUT_DIR}/")
        print(f"ğŸ“Š ã‚°ãƒ©ãƒ•ç”»åƒ: {OUTPUT_DIR}/*.png")
        print("=" * 60)


# ==============================================================================
# 5. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# ==============================================================================

def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    é¢æ¥éŸ³å£°åˆ†æã®å…¨ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™
    """
    print("ğŸ™ï¸  é¢æ¥éŸ³å£°åˆ†æãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹")
    print("=" * 60)
    
    # === éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª ===
    if not AUDIO_M4A.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"   {AUDIO_M4A} ã«é¢æ¥éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆm4aå½¢å¼ï¼‰ã‚’é…ç½®ã—ã¦ãã ã•ã„")
        sys.exit(1)
    
    # === åˆ†æå™¨ã®åˆæœŸåŒ– ===
    analyzer = InterviewSpeechAnalyzer()
    
    # === ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµæœã®ç¢ºèª ===
    segments = analyzer.load_cached_results()
    
    if segments is None:
        # === æ–°è¦åˆ†æã®å®Ÿè¡Œ ===
        print("ğŸ”„ æ–°è¦åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’WAVå½¢å¼ã«å¤‰æ›
        analyzer.convert_audio_to_wav(AUDIO_M4A, WAV_TMP)
        
        # Whisperã§éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—
        segments = analyzer.transcribe_audio_with_whisper(WAV_TMP)
        
        # è©±è€…è­˜åˆ¥ï¼ˆé¢æ¥å®˜ vs é¢æ¥è€…ï¼‰
        segments = analyzer.identify_speakers(WAV_TMP, segments)
        
        # å„ç™ºè©±ã®è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        print("â‘£ è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
        start_time = time.time()
        for segment in segments:
            segment["metrics"] = analyzer.analyze_speech_patterns(segment["text"])
        
        duration = time.time() - start_time
        print(f"   âœ“ è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº† ({duration:.1f}ç§’)\n")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        try:
            WAV_TMP.unlink()
        except FileNotFoundError:
            pass
    
    else:
        # === ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ ===
        print("âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµæœã‚’ä½¿ç”¨ã—ã¾ã™")
    
    # === è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®— ===
    metrics = analyzer.calculate_evaluation_metrics(segments)
    
    if not metrics:
        print("âŒ ã‚¨ãƒ©ãƒ¼: è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    # === å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã®ä½œæˆ ===
    analyzer.create_visualization_charts(segments, metrics)
    
    # === çµæœã®ä¿å­˜ ===
    analyzer.save_results_to_files(segments, metrics)
    
    # === çµæœã®è¡¨ç¤º ===
    analyzer.display_results(metrics, segments)


# ==============================================================================
# 6. ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œéƒ¨åˆ†
# ==============================================================================

if __name__ == "__main__":
    """
    ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    
    ä½¿ç”¨æ–¹æ³•:
        python interview_analyzer.py
    
    å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«:
        - input/interview.m4a: åˆ†æå¯¾è±¡ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«
        - database/speech_patterns.yaml: è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«
    
    å‡ºåŠ›:
        - output/*.png: å¯è¦–åŒ–ã‚°ãƒ©ãƒ•
        - output/*.csv: åˆ†æçµæœãƒ‡ãƒ¼ã‚¿
        - cache/analysis_cache.csv: æ¬¡å›é«˜é€Ÿå®Ÿè¡Œç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    """
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
        import traceback
        traceback.print_exc()
        sys.exit(1) 